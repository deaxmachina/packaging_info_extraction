{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR on packaging using Google Vison API - can we extract ingredients or nutritional information? \n",
    "\n",
    "Initial exploratory work using the Google Vision API - text detection and document detection. \n",
    "- Attempted to use the text and document detection functionality out of the box but it proved to be imperfect. \n",
    "- Experimented with weather paragraph or block detection works better using the document detection API. \n",
    "- Tests on whole package images and on photos of just the ingredients for the Open Food Facts db. \n",
    "- Simple flow (WIP) to extract the list of ingredients. \n",
    "- Various issues encountered and possible solutions documented in the notes file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "from enum import Enum\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Instantiates a client\n",
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some test images: Those in the 'test_images/whole' folder contain photos of the whole side of the packaging and those in 'test_images/partial' contain just the part we're interested, i.e. just ingredients or just nutrition etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_whole = {}\n",
    "for img in os.listdir('test_images/whole'):\n",
    "    tests_whole[img.split('.')[0]] = os.path.join('test_images/whole', img)\n",
    "\n",
    "tests_partial = {}\n",
    "for img in os.listdir('test_images/partial'):\n",
    "    tests_partial[img.split('.')[0]] = os.path.join('test_images/partial', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: Google text detection API \n",
    "- Using this example: https://cloud.google.com/vision/docs/detecting-text#vision-text-detection-python\n",
    "- Not very useful in this case as it simply spits out all the text. In the case of multiple columns or text boxes etc. it doesn't group the text correctly but reads it horizontally. Note that the same propblem happens with the document detection API, too (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      "\n",
      "\"200\n",
      "f an honds, Brazil nuts, cashews and wairu's\n",
      "EGAN\n",
      "or best before end: see ba\n",
      "Store in a cool dry place. Once\n",
      "tain freshness\n",
      "Great for all ofu\n",
      "|\n",
      "Source of Protein: Protek\n",
      "the growth and mai\n",
      "muscle mass.\n",
      "as part of a\n",
      "for best betoilypla\n",
      "Enjowced diet and a\n",
      "Great to know\n",
      "utrition\n",
      "values per 100g per 30g\n",
      "Iladul RI\n",
      "serving Der 300\n",
      "Not Yet Recycled\n",
      "2665 799\n",
      "We're sure you't love this\n",
      "product. li you don't,simpl\n",
      "return for afull refund\n",
      "Or, call our careline 080522\n",
      "Your statutory rights are not affected\n",
      "Produce of more than one county\n",
      "Packed in the UK for Sainsbus\n",
      "Supermarkets Ltd, London ECIYCH\n",
      "Packaged in a protectiveamaphee\n",
      "8100\n",
      "193\n",
      "644\n",
      "ukcal\n",
      "170\n",
      "Try me!\n",
      "70\n",
      "24\n",
      "mono-tantes 240g 72g\n",
      "olyunsatuas 219g 6.6g\n",
      "20g\n",
      "1% 260g\n",
      "196\n",
      "ich su\n",
      "16 90g\n",
      "bre\n",
      "1.99\n",
      "6.3q\n",
      "01g\n",
      "Reference Intakes of an aeage adult (8400kJ72000kcal)\n",
      "ingredients. Almonds (25 %), Brazil Nuts (25%),\n",
      "ew Nut (2590), Walnut Halves (25%).\n",
      "otein\n",
      "13%\n",
      "50g\n",
      "Want to find out more?\n",
      "sainsburus.co.uk\n",
      "0518\n",
      "1060768\n",
      "llergy\n",
      "advice For allergens, see ingredients\n",
      "Also, not suitable for customers with an\n",
      "to other nuts or sesame due to\n",
      "cturing methods\n",
      "aory carehas been taken to\n",
      "all\n",
      "shell, some may remain. Remember\n",
      "children can choke on nuts.\n",
      "K0106 0768\n",
      "Best before end\n",
      "Dec 2019\n",
      "\"\n",
      "bounds: (64,598),(2753,598),(2753,3962),(64,3962)\n",
      "\n",
      "\"200\"\n",
      "bounds: (2600,604),(2748,598),(2751,674),(2603,681)\n",
      "\n",
      "\"f\"\n",
      "bounds: (726,801),(752,800),(753,844),(727,845)\n",
      "\n",
      "\"an\"\n",
      "bounds: (765,811),(846,809),(847,853),(766,855)\n",
      "\n",
      "\"honds,\"\n",
      "bounds: (858,799),(1067,795),(1068,868),(859,872)\n",
      "\n",
      "\"Brazil\"\n",
      "bounds: (1086,799),(1266,795),(1267,858),(1087,862)\n",
      "\n",
      "\"nuts,\"\n",
      "bounds: (1283,801),(1453,798),(1454,863),(1284,866)\n",
      "\n",
      "\"cashews\"\n",
      "bounds: (1476,791),(1742,786),(1743,859),(1477,864)\n",
      "\n",
      "\"and\"\n",
      "bounds: (1760,779),(1884,777),(1885,842),(1761,844)\n",
      "\n",
      "\"wairu's\"\n",
      "bounds: (1903,771),(2171,766),(2172,829),(1904,834)\n",
      "\n",
      "\"EGAN\"\n",
      "bounds: (64,921),(163,903),(170,941),(71,958)\n",
      "\n",
      "\"or\"\n",
      "bounds: (66,1086),(123,1074),(139,1154),(82,1165)\n",
      "\n",
      "\"best\"\n",
      "bounds: (151,1069),(256,1048),(272,1127),(167,1148)\n",
      "\n",
      "\"before\"\n",
      "bounds: (273,1045),(406,1018),(422,1097),(289,1124)\n",
      "\n",
      "\"end:\"\n",
      "bounds: (419,1013),(500,997),(516,1076),(435,1092)\n",
      "\n",
      "\"see\"\n",
      "bounds: (515,996),(594,980),(610,1059),(531,1075)\n",
      "\n",
      "\"ba\"\n",
      "bounds: (610,976),(663,965),(679,1045),(626,1055)\n",
      "\n",
      "\"Store\"\n",
      "bounds: (84,1163),(171,1145),(184,1207),(97,1225)\n",
      "\n",
      "\"in\"\n",
      "bounds: (194,1139),(225,1133),(238,1194),(207,1201)\n",
      "\n",
      "\"a\"\n",
      "bounds: (248,1130),(266,1126),(278,1188),(261,1192)\n",
      "\n",
      "\"cool\"\n",
      "bounds: (291,1120),(362,1105),(375,1167),(304,1182)\n",
      "\n",
      "\"dry\"\n",
      "bounds: (376,1102),(417,1094),(430,1155),(389,1164)\n",
      "\n",
      "\"place.\"\n",
      "bounds: (439,1090),(534,1070),(547,1132),(452,1152)\n",
      "\n",
      "\"Once\"\n",
      "bounds: (545,1069),(662,1045),(674,1107),(558,1131)\n",
      "\n",
      "\"tain\"\n",
      "bounds: (129,1220),(211,1206),(218,1249),(137,1263)\n",
      "\n",
      "\"freshness\"\n",
      "bounds: (228,1206),(393,1177),(402,1224),(236,1253)\n",
      "\n",
      "\"Great\"\n",
      "bounds: (1653,1006),(1957,967),(1966,1044),(1663,1082)\n",
      "\n",
      "\"for\"\n",
      "bounds: (1982,960),(2155,938),(2163,1005),(1990,1026)\n",
      "\n",
      "\"all\"\n",
      "bounds: (2181,931),(2340,911),(2349,987),(2191,1007)\n",
      "\n",
      "\"ofu\"\n",
      "bounds: (2368,913),(2581,886),(2594,982),(2380,1009)\n",
      "\n",
      "\"|\"\n",
      "bounds: (1541,1013),(1563,1010),(1576,1116),(1555,1119)\n",
      "\n",
      "\"Source\"\n",
      "bounds: (1850,1096),(2052,1078),(2056,1122),(1854,1140)\n",
      "\n",
      "\"of\"\n",
      "bounds: (2065,1059),(2128,1054),(2132,1103),(2069,1109)\n",
      "\n",
      "\"Protein:\"\n",
      "bounds: (2141,1053),(2390,1031),(2395,1083),(2146,1105)\n",
      "\n",
      "\"Protek\"\n",
      "bounds: (2405,1041),(2538,1029),(2544,1092),(2410,1104)\n",
      "\n",
      "\"the\"\n",
      "bounds: (1854,1161),(1934,1152),(1939,1192),(1858,1201)\n",
      "\n",
      "\"growth\"\n",
      "bounds: (1943,1143),(2133,1122),(2140,1181),(1950,1203)\n",
      "\n",
      "\"and\"\n",
      "bounds: (2141,1116),(2249,1104),(2254,1150),(2146,1162)\n",
      "\n",
      "\"mai\"\n",
      "bounds: (2266,1112),(2370,1100),(2376,1148),(2271,1160)\n",
      "\n",
      "\"muscle\"\n",
      "bounds: (1848,1224),(2035,1196),(2042,1243),(1855,1271)\n",
      "\n",
      "\"mass.\"\n",
      "bounds: (2051,1206),(2203,1183),(2208,1217),(2056,1240)\n",
      "\n",
      "\"as\"\n",
      "bounds: (2008,1277),(2067,1267),(2073,1300),(2014,1310)\n",
      "\n",
      "\"part\"\n",
      "bounds: (2078,1256),(2187,1237),(2196,1288),(2087,1307)\n",
      "\n",
      "\"of\"\n",
      "bounds: (2203,1236),(2260,1226),(2268,1269),(2211,1279)\n",
      "\n",
      "\"a\"\n",
      "bounds: (2275,1242),(2307,1236),(2313,1272),(2281,1277)\n",
      "\n",
      "\"for\"\n",
      "bounds: (68,1076),(135,1074),(138,1165),(71,1167)\n",
      "\n",
      "\"best\"\n",
      "bounds: (151,1074),(260,1071),(263,1162),(154,1165)\n",
      "\n",
      "\"betoilypla\"\n",
      "bounds: (291,1070),(492,1064),(495,1155),(294,1161)\n",
      "\n",
      "\"Enjowced\"\n",
      "bounds: (1852,1265),(2081,1275),(2076,1376),(1848,1366)\n",
      "\n",
      "\"diet\"\n",
      "bounds: (2100,1275),(2203,1279),(2198,1380),(2096,1376)\n",
      "\n",
      "\"and\"\n",
      "bounds: (2222,1281),(2305,1285),(2301,1386),(2218,1382)\n",
      "\n",
      "\"a\"\n",
      "bounds: (2342,1287),(2358,1288),(2354,1389),(2338,1388)\n",
      "\n",
      "\"Great\"\n",
      "bounds: (120,1324),(316,1289),(329,1365),(133,1400)\n",
      "\n",
      "\"to\"\n",
      "bounds: (328,1283),(402,1270),(412,1330),(339,1343)\n",
      "\n",
      "\"know\"\n",
      "bounds: (423,1250),(695,1202),(712,1302),(441,1349)\n",
      "\n",
      "\"utrition\"\n",
      "bounds: (131,1421),(262,1421),(262,1474),(131,1474)\n",
      "\n",
      "\"values\"\n",
      "bounds: (212,1441),(305,1435),(310,1510),(217,1516)\n",
      "\n",
      "\"per\"\n",
      "bounds: (358,1431),(435,1426),(440,1501),(363,1506)\n",
      "\n",
      "\"100g\"\n",
      "bounds: (450,1425),(551,1418),(556,1493),(455,1500)\n",
      "\n",
      "\"per\"\n",
      "bounds: (592,1415),(681,1409),(686,1484),(597,1490)\n",
      "\n",
      "\"30g\"\n",
      "bounds: (706,1407),(783,1402),(788,1477),(711,1482)\n",
      "\n",
      "\"Iladul\"\n",
      "bounds: (1063,1425),(1308,1436),(1306,1486),(1061,1475)\n",
      "\n",
      "\"RI\"\n",
      "bounds: (1334,1437),(1394,1440),(1392,1490),(1332,1487)\n",
      "\n",
      "\"serving\"\n",
      "bounds: (624,1482),(787,1482),(787,1531),(624,1531)\n",
      "\n",
      "\"Der\"\n",
      "bounds: (820,1482),(1014,1482),(1014,1531),(820,1531)\n",
      "\n",
      "\"300\"\n",
      "bounds: (1033,1482),(1109,1482),(1109,1531),(1033,1531)\n",
      "\n",
      "\"Not\"\n",
      "bounds: (1801,1506),(1889,1491),(1897,1536),(1809,1551)\n",
      "\n",
      "\"Yet\"\n",
      "bounds: (1903,1492),(1975,1479),(1982,1521),(1910,1533)\n",
      "\n",
      "\"Recycled\"\n",
      "bounds: (1988,1472),(2202,1434),(2212,1493),(1998,1531)\n",
      "\n",
      "\"2665\"\n",
      "bounds: (450,1604),(561,1599),(563,1647),(452,1652)\n",
      "\n",
      "\"799\"\n",
      "bounds: (706,1588),(791,1584),(793,1630),(708,1634)\n",
      "\n",
      "\"We're\"\n",
      "bounds: (2071,1594),(2190,1601),(2186,1668),(2067,1661)\n",
      "\n",
      "\"sure\"\n",
      "bounds: (2216,1602),(2272,1605),(2268,1672),(2212,1669)\n",
      "\n",
      "\"you't\"\n",
      "bounds: (2295,1608),(2376,1613),(2372,1680),(2291,1675)\n",
      "\n",
      "\"love\"\n",
      "bounds: (2390,1614),(2446,1617),(2442,1684),(2386,1681)\n",
      "\n",
      "\"this\"\n",
      "bounds: (2458,1618),(2527,1622),(2523,1689),(2454,1685)\n",
      "\n",
      "\"product.\"\n",
      "bounds: (2051,1665),(2258,1674),(2255,1739),(2048,1730)\n",
      "\n",
      "\"li\"\n",
      "bounds: (2271,1675),(2291,1676),(2288,1741),(2268,1740)\n",
      "\n",
      "\"you\"\n",
      "bounds: (2305,1677),(2349,1679),(2346,1744),(2302,1742)\n",
      "\n",
      "\"don't,simpl\"\n",
      "bounds: (2374,1679),(2542,1686),(2539,1751),(2371,1744)\n",
      "\n",
      "\"return\"\n",
      "bounds: (2053,1744),(2208,1744),(2208,1806),(2053,1806)\n",
      "\n",
      "\"for\"\n",
      "bounds: (2218,1744),(2280,1744),(2280,1806),(2218,1806)\n",
      "\n",
      "\"afull\"\n",
      "bounds: (2293,1744),(2379,1744),(2379,1806),(2293,1806)\n",
      "\n",
      "\"refund\"\n",
      "bounds: (2392,1744),(2476,1744),(2476,1806),(2392,1806)\n",
      "\n",
      "\"Or,\"\n",
      "bounds: (2053,1811),(2117,1811),(2117,1872),(2053,1872)\n",
      "\n",
      "\"call\"\n",
      "bounds: (2140,1811),(2222,1811),(2222,1872),(2140,1872)\n",
      "\n",
      "\"our\"\n",
      "bounds: (2234,1811),(2295,1811),(2295,1872),(2234,1872)\n",
      "\n",
      "\"careline\"\n",
      "bounds: (2317,1811),(2431,1811),(2431,1872),(2317,1872)\n",
      "\n",
      "\"080522\"\n",
      "bounds: (2443,1811),(2555,1811),(2555,1872),(2443,1872)\n",
      "\n",
      "\"Your\"\n",
      "bounds: (1649,1903),(1756,1898),(1759,1973),(1652,1978)\n",
      "\n",
      "\"statutory\"\n",
      "bounds: (1767,1897),(2010,1886),(2013,1961),(1770,1972)\n",
      "\n",
      "\"rights\"\n",
      "bounds: (2037,1886),(2191,1879),(2194,1954),(2040,1961)\n",
      "\n",
      "\"are\"\n",
      "bounds: (2204,1880),(2267,1877),(2270,1952),(2207,1955)\n",
      "\n",
      "\"not\"\n",
      "bounds: (2281,1876),(2331,1874),(2334,1949),(2284,1951)\n",
      "\n",
      "\"affected\"\n",
      "bounds: (2346,1874),(2459,1869),(2462,1944),(2349,1949)\n",
      "\n",
      "\"Produce\"\n",
      "bounds: (1647,1992),(1852,1983),(1855,2052),(1650,2061)\n",
      "\n",
      "\"of\"\n",
      "bounds: (1866,1982),(1924,1979),(1927,2048),(1869,2051)\n",
      "\n",
      "\"more\"\n",
      "bounds: (1937,1980),(2077,1974),(2080,2043),(1940,2049)\n",
      "\n",
      "\"than\"\n",
      "bounds: (2104,1972),(2209,1967),(2212,2036),(2107,2041)\n",
      "\n",
      "\"one\"\n",
      "bounds: (2234,1966),(2292,1963),(2295,2032),(2237,2035)\n",
      "\n",
      "\"county\"\n",
      "bounds: (2305,1962),(2398,1958),(2401,2027),(2308,2031)\n",
      "\n",
      "\"Packed\"\n",
      "bounds: (1647,2063),(1817,2056),(1820,2121),(1650,2128)\n",
      "\n",
      "\"in\"\n",
      "bounds: (1840,2055),(1884,2053),(1887,2118),(1843,2120)\n",
      "\n",
      "\"the\"\n",
      "bounds: (1907,2051),(1996,2047),(1999,2112),(1910,2116)\n",
      "\n",
      "\"UK\"\n",
      "bounds: (2019,2047),(2084,2044),(2087,2109),(2022,2112)\n",
      "\n",
      "\"for\"\n",
      "bounds: (2110,2043),(2175,2040),(2178,2105),(2113,2108)\n",
      "\n",
      "\"Sainsbus\"\n",
      "bounds: (2199,2039),(2365,2032),(2368,2097),(2202,2104)\n",
      "\n",
      "\"Supermarkets\"\n",
      "bounds: (1651,2126),(2012,2110),(2015,2179),(1654,2195)\n",
      "\n",
      "\"Ltd,\"\n",
      "bounds: (2037,2108),(2134,2104),(2137,2173),(2040,2177)\n",
      "\n",
      "\"London\"\n",
      "bounds: (2161,2104),(2293,2098),(2296,2167),(2164,2173)\n",
      "\n",
      "\"ECIYCH\"\n",
      "bounds: (2309,2096),(2392,2092),(2395,2161),(2312,2165)\n",
      "\n",
      "\"Packaged\"\n",
      "bounds: (1653,2206),(1892,2196),(1896,2286),(1657,2297)\n",
      "\n",
      "\"in\"\n",
      "bounds: (1923,2195),(1971,2193),(1975,2284),(1927,2286)\n",
      "\n",
      "\"a\"\n",
      "bounds: (1986,2193),(2016,2192),(2020,2283),(1990,2284)\n",
      "\n",
      "\"protectiveamaphee\"\n",
      "bounds: (2035,2191),(2423,2174),(2427,2265),(2039,2282)\n",
      "\n",
      "\"8100\"\n",
      "bounds: (1275,1610),(1398,1605),(1400,1651),(1277,1656)\n",
      "\n",
      "\"193\"\n",
      "bounds: (712,1651),(792,1651),(792,1698),(712,1698)\n",
      "\n",
      "\"644\"\n",
      "bounds: (478,1665),(558,1665),(558,1714),(478,1714)\n",
      "\n",
      "\"ukcal\"\n",
      "bounds: (192,1706),(275,1689),(285,1734),(201,1751)\n",
      "\n",
      "\"170\"\n",
      "bounds: (673,1724),(752,1717),(756,1761),(677,1768)\n",
      "\n",
      "\"Try\"\n",
      "bounds: (1683,1722),(1802,1712),(1810,1804),(1691,1815)\n",
      "\n",
      "\"me!\"\n",
      "bounds: (1821,1697),(1972,1684),(1979,1764),(1828,1778)\n",
      "\n",
      "\"70\"\n",
      "bounds: (1311,1736),(1366,1736),(1366,1779),(1311,1779)\n",
      "\n",
      "\"24\"\n",
      "bounds: (683,1852),(748,1849),(750,1895),(685,1898)\n",
      "\n",
      "\"mono-tantes\"\n",
      "bounds: (208,1925),(432,1925),(432,1980),(208,1980)\n",
      "\n",
      "\"240g\"\n",
      "bounds: (442,1925),(528,1925),(528,1980),(442,1980)\n",
      "\n",
      "\"72g\"\n",
      "bounds: (578,1925),(770,1925),(770,1980),(578,1980)\n",
      "\n",
      "\"olyunsatuas\"\n",
      "bounds: (212,1982),(424,1982),(424,2046),(212,2046)\n",
      "\n",
      "\"219g\"\n",
      "bounds: (448,1982),(526,1982),(526,2046),(448,2046)\n",
      "\n",
      "\"6.6g\"\n",
      "bounds: (584,1982),(767,1982),(767,2046),(584,2046)\n",
      "\n",
      "\"20g\"\n",
      "bounds: (1307,1858),(1398,1866),(1393,1920),(1302,1912)\n",
      "\n",
      "\"1%\"\n",
      "bounds: (1033,2047),(1096,2048),(1095,2108),(1032,2107)\n",
      "\n",
      "\"260g\"\n",
      "bounds: (1128,2049),(1380,2054),(1379,2114),(1127,2109)\n",
      "\n",
      "\"196\"\n",
      "bounds: (1033,2049),(1103,2049),(1103,2096),(1033,2096)\n",
      "\n",
      "\"ich\"\n",
      "bounds: (255,2110),(311,2113),(309,2155),(253,2152)\n",
      "\n",
      "\"su\"\n",
      "bounds: (320,2122),(370,2124),(368,2156),(319,2154)\n",
      "\n",
      "\"16\"\n",
      "bounds: (891,2098),(1092,2107),(1088,2190),(887,2181)\n",
      "\n",
      "\"90g\"\n",
      "bounds: (1179,2110),(1374,2119),(1370,2201),(1175,2193)\n",
      "\n",
      "\"bre\"\n",
      "bounds: (185,2175),(235,2177),(233,2211),(184,2209)\n",
      "\n",
      "\"1.99\"\n",
      "bounds: (635,2185),(758,2190),(755,2250),(632,2245)\n",
      "\n",
      "\"6.3q\"\n",
      "bounds: (675,2246),(754,2253),(749,2303),(671,2296)\n",
      "\n",
      "\"01g\"\n",
      "bounds: (692,2315),(765,2321),(760,2379),(687,2373)\n",
      "\n",
      "\"Reference\"\n",
      "bounds: (183,2360),(376,2368),(372,2459),(179,2451)\n",
      "\n",
      "\"Intakes\"\n",
      "bounds: (395,2370),(519,2375),(515,2466),(391,2461)\n",
      "\n",
      "\"of\"\n",
      "bounds: (537,2376),(567,2377),(563,2468),(533,2467)\n",
      "\n",
      "\"an\"\n",
      "bounds: (584,2378),(616,2379),(612,2470),(580,2469)\n",
      "\n",
      "\"aeage\"\n",
      "bounds: (631,2380),(755,2385),(751,2476),(627,2471)\n",
      "\n",
      "\"adult\"\n",
      "bounds: (775,2386),(899,2391),(895,2482),(771,2477)\n",
      "\n",
      "\"(8400kJ72000kcal)\"\n",
      "bounds: (917,2392),(1376,2412),(1372,2503),(913,2483)\n",
      "\n",
      "\"ingredients\"\n",
      "bounds: (200,2478),(445,2494),(441,2551),(196,2536)\n",
      "\n",
      "\".\"\n",
      "bounds: (446,2498),(460,2499),(457,2541),(443,2540)\n",
      "\n",
      "\"Almonds\"\n",
      "bounds: (478,2496),(622,2505),(618,2561),(474,2552)\n",
      "\n",
      "\"(\"\n",
      "bounds: (631,2518),(647,2519),(644,2571),(628,2570)\n",
      "\n",
      "\"25\"\n",
      "bounds: (649,2519),(685,2521),(682,2573),(646,2571)\n",
      "\n",
      "\"%\"\n",
      "bounds: (689,2523),(713,2525),(710,2570),(686,2569)\n",
      "\n",
      "\")\"\n",
      "bounds: (710,2529),(720,2530),(717,2580),(707,2579)\n",
      "\n",
      "\",\"\n",
      "bounds: (722,2529),(732,2530),(729,2580),(719,2579)\n",
      "\n",
      "\"Brazil\"\n",
      "bounds: (742,2518),(896,2528),(892,2586),(738,2576)\n",
      "\n",
      "\"Nuts\"\n",
      "bounds: (909,2525),(1041,2533),(1038,2583),(906,2575)\n",
      "\n",
      "\"(\"\n",
      "bounds: (1057,2527),(1081,2529),(1077,2588),(1053,2587)\n",
      "\n",
      "\"25\"\n",
      "bounds: (1082,2527),(1142,2531),(1138,2592),(1078,2588)\n",
      "\n",
      "\"%\"\n",
      "bounds: (1143,2531),(1181,2533),(1178,2581),(1140,2579)\n",
      "\n",
      "\")\"\n",
      "bounds: (1181,2529),(1197,2530),(1193,2590),(1177,2589)\n",
      "\n",
      "\",\"\n",
      "bounds: (1198,2529),(1216,2530),(1212,2590),(1194,2589)\n",
      "\n",
      "\"ew\"\n",
      "bounds: (196,2549),(254,2555),(250,2595),(192,2589)\n",
      "\n",
      "\"Nut\"\n",
      "bounds: (261,2547),(344,2555),(339,2597),(257,2589)\n",
      "\n",
      "\"(\"\n",
      "bounds: (356,2559),(368,2560),(363,2606),(351,2605)\n",
      "\n",
      "\"2590\"\n",
      "bounds: (368,2553),(463,2563),(458,2608),(363,2599)\n",
      "\n",
      "\")\"\n",
      "bounds: (464,2565),(474,2566),(469,2614),(459,2613)\n",
      "\n",
      "\",\"\n",
      "bounds: (476,2565),(486,2566),(481,2614),(471,2613)\n",
      "\n",
      "\"Walnut\"\n",
      "bounds: (498,2567),(627,2580),(622,2628),(493,2615)\n",
      "\n",
      "\"Halves\"\n",
      "bounds: (631,2586),(751,2598),(747,2644),(626,2632)\n",
      "\n",
      "\"(\"\n",
      "bounds: (761,2596),(775,2597),(769,2653),(755,2652)\n",
      "\n",
      "\"25\"\n",
      "bounds: (777,2596),(829,2601),(824,2651),(772,2646)\n",
      "\n",
      "\"%\"\n",
      "bounds: (828,2598),(874,2603),(869,2648),(823,2644)\n",
      "\n",
      "\")\"\n",
      "bounds: (874,2598),(892,2600),(886,2661),(868,2659)\n",
      "\n",
      "\".\"\n",
      "bounds: (889,2636),(901,2637),(900,2649),(888,2648)\n",
      "\n",
      "\"otein\"\n",
      "bounds: (181,2226),(262,2230),(260,2275),(179,2272)\n",
      "\n",
      "\"13\"\n",
      "bounds: (996,2248),(1050,2248),(1050,2298),(996,2298)\n",
      "\n",
      "\"%\"\n",
      "bounds: (1049,2250),(1085,2250),(1085,2296),(1049,2296)\n",
      "\n",
      "\"50g\"\n",
      "bounds: (1297,2250),(1389,2250),(1389,2312),(1297,2312)\n",
      "\n",
      "\"Want\"\n",
      "bounds: (1878,2330),(2011,2318),(2018,2389),(1884,2401)\n",
      "\n",
      "\"to\"\n",
      "bounds: (2041,2317),(2087,2313),(2093,2384),(2047,2388)\n",
      "\n",
      "\"find\"\n",
      "bounds: (2112,2309),(2209,2301),(2215,2371),(2118,2380)\n",
      "\n",
      "\"out\"\n",
      "bounds: (2220,2301),(2270,2297),(2276,2367),(2226,2372)\n",
      "\n",
      "\"more?\"\n",
      "bounds: (2281,2295),(2366,2288),(2372,2358),(2287,2366)\n",
      "\n",
      "\"sainsburus.co.uk\"\n",
      "bounds: (1874,2401),(2373,2357),(2380,2434),(1881,2478)\n",
      "\n",
      "\"0518\"\n",
      "bounds: (2206,2543),(2305,2534),(2310,2594),(2211,2603)\n",
      "\n",
      "\"1060768\"\n",
      "bounds: (1730,2573),(1963,2563),(1965,2617),(1732,2627)\n",
      "\n",
      "\"llergy\"\n",
      "bounds: (259,2636),(384,2648),(377,2727),(251,2715)\n",
      "\n",
      "\"advice\"\n",
      "bounds: (413,2655),(552,2667),(546,2744),(406,2732)\n",
      "\n",
      "\"For\"\n",
      "bounds: (582,2669),(620,2672),(613,2749),(575,2746)\n",
      "\n",
      "\"allergens,\"\n",
      "bounds: (637,2675),(825,2691),(819,2768),(630,2752)\n",
      "\n",
      "\"see\"\n",
      "bounds: (838,2693),(907,2699),(900,2776),(831,2770)\n",
      "\n",
      "\"ingredients\"\n",
      "bounds: (921,2701),(1220,2727),(1213,2804),(914,2778)\n",
      "\n",
      "\"Also,\"\n",
      "bounds: (399,2730),(500,2739),(495,2793),(394,2784)\n",
      "\n",
      "\"not\"\n",
      "bounds: (513,2744),(596,2751),(592,2797),(509,2790)\n",
      "\n",
      "\"suitable\"\n",
      "bounds: (600,2752),(749,2765),(745,2821),(595,2808)\n",
      "\n",
      "\"for\"\n",
      "bounds: (752,2775),(806,2780),(801,2832),(747,2827)\n",
      "\n",
      "\"customers\"\n",
      "bounds: (813,2781),(1078,2804),(1073,2856),(808,2833)\n",
      "\n",
      "\"with\"\n",
      "bounds: (1092,2791),(1209,2801),(1204,2853),(1087,2843)\n",
      "\n",
      "\"an\"\n",
      "bounds: (1220,2803),(1291,2809),(1287,2855),(1216,2849)\n",
      "\n",
      "\"to\"\n",
      "bounds: (372,2801),(414,2806),(409,2848),(367,2843)\n",
      "\n",
      "\"other\"\n",
      "bounds: (425,2799),(548,2813),(542,2863),(419,2849)\n",
      "\n",
      "\"nuts\"\n",
      "bounds: (559,2815),(661,2827),(656,2872),(554,2861)\n",
      "\n",
      "\"or\"\n",
      "bounds: (669,2834),(707,2838),(703,2874),(665,2870)\n",
      "\n",
      "\"sesame\"\n",
      "bounds: (716,2842),(857,2858),(852,2904),(711,2888)\n",
      "\n",
      "\"due\"\n",
      "bounds: (870,2854),(964,2865),(958,2916),(864,2906)\n",
      "\n",
      "\"to\"\n",
      "bounds: (976,2862),(1030,2868),(1024,2916),(971,2910)\n",
      "\n",
      "\"cturing\"\n",
      "bounds: (376,2866),(529,2879),(524,2940),(371,2927)\n",
      "\n",
      "\"methods\"\n",
      "bounds: (545,2880),(747,2898),(743,2949),(540,2932)\n",
      "\n",
      "\"aory\"\n",
      "bounds: (379,2949),(696,2956),(694,3059),(377,3052)\n",
      "\n",
      "\"carehas\"\n",
      "bounds: (716,2957),(872,2960),(870,3063),(714,3060)\n",
      "\n",
      "\"been\"\n",
      "bounds: (891,2960),(1010,2963),(1008,3066),(889,3063)\n",
      "\n",
      "\"taken\"\n",
      "bounds: (1029,2962),(1167,2965),(1165,3068),(1027,3065)\n",
      "\n",
      "\"to\"\n",
      "bounds: (1187,2966),(1239,2967),(1237,3070),(1185,3069)\n",
      "\n",
      "\"all\"\n",
      "bounds: (381,2996),(431,3002),(425,3048),(375,3042)\n",
      "\n",
      "\"shell,\"\n",
      "bounds: (442,3004),(546,3015),(540,3071),(436,3060)\n",
      "\n",
      "\"some\"\n",
      "bounds: (557,3029),(682,3043),(678,3080),(553,3067)\n",
      "\n",
      "\"may\"\n",
      "bounds: (694,3045),(796,3056),(791,3108),(688,3097)\n",
      "\n",
      "\"remain.\"\n",
      "bounds: (805,3059),(970,3077),(965,3127),(800,3109)\n",
      "\n",
      "\"Remember\"\n",
      "bounds: (982,3063),(1278,3095),(1272,3158),(975,3126)\n",
      "\n",
      "\"children\"\n",
      "bounds: (352,3055),(506,3079),(499,3122),(345,3098)\n",
      "\n",
      "\"can\"\n",
      "bounds: (515,3090),(591,3102),(586,3137),(509,3126)\n",
      "\n",
      "\"choke\"\n",
      "bounds: (602,3090),(740,3111),(733,3161),(594,3139)\n",
      "\n",
      "\"on\"\n",
      "bounds: (752,3120),(809,3129),(804,3166),(746,3158)\n",
      "\n",
      "\"nuts.\"\n",
      "bounds: (818,3132),(920,3148),(913,3191),(811,3175)\n",
      "\n",
      "\"K0106\"\n",
      "bounds: (1675,3140),(1993,3112),(2004,3235),(1686,3263)\n",
      "\n",
      "\"0768\"\n",
      "bounds: (2059,3106),(2285,3086),(2296,3209),(2070,3229)\n",
      "\n",
      "\"Best\"\n",
      "bounds: (1072,3677),(1206,3683),(1203,3744),(1069,3738)\n",
      "\n",
      "\"before\"\n",
      "bounds: (1220,3689),(1425,3698),(1422,3761),(1217,3752)\n",
      "\n",
      "\"end\"\n",
      "bounds: (1439,3697),(1565,3702),(1562,3765),(1436,3760)\n",
      "\n",
      "\"Dec\"\n",
      "bounds: (1317,3890),(1474,3890),(1474,3962),(1317,3962)\n",
      "\n",
      "\"2019\"\n",
      "bounds: (1500,3886),(1700,3886),(1700,3960),(1500,3960)\n"
     ]
    }
   ],
   "source": [
    "# test with examples to get a feel for the output\n",
    "detect_text(tests_whole['test_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Google document detection API \n",
    "- Using (and modifying) this example: https://cloud.google.com/vision/docs/fulltext-annotations\n",
    "- This doesn't work very well when the layout is with multiple columns (see notes above). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper fuctions for visualising bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureType(Enum):\n",
    "    PAGE = 1\n",
    "    BLOCK = 2\n",
    "    PARA = 3\n",
    "    WORD = 4\n",
    "    SYMBOL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, bounds, color):\n",
    "    \"\"\"Draw a border around the image using the hints in the vector list.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for bound in bounds:\n",
    "        draw.polygon([\n",
    "            bound.vertices[0].x, bound.vertices[0].y,\n",
    "            bound.vertices[1].x, bound.vertices[1].y,\n",
    "            bound.vertices[2].x, bound.vertices[2].y,\n",
    "            bound.vertices[3].x, bound.vertices[3].y], None, color)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_doc_text(filein, fileout, block=True, para=False, word=False):\n",
    "    image = Image.open(filein)\n",
    "    if block: \n",
    "        bounds = get_document_bounds(filein, FeatureType.BLOCK)\n",
    "        draw_boxes(image, bounds, 'blue')\n",
    "    if para: \n",
    "        bounds = get_document_bounds(filein, FeatureType.PARA)\n",
    "        draw_boxes(image, bounds, 'red')\n",
    "    if word: \n",
    "        bounds = get_document_bounds(filein, FeatureType.WORD)\n",
    "        draw_boxes(image, bounds, 'yellow')\n",
    "\n",
    "    if fileout is not 0:\n",
    "        image.save(fileout)\n",
    "    else:\n",
    "        image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detect document i.e. all the parts, and draw them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_bounds(image_file, feature):\n",
    "    \"\"\"Returns document bounds given an image.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    bounds = []\n",
    "\n",
    "    with io.open(image_file, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = types.Image(content=content)\n",
    "\n",
    "    response = client.document_text_detection(image=image)\n",
    "    document = response.full_text_annotation\n",
    "\n",
    "    # Collect specified feature bounds by enumerating all document features\n",
    "    for page in document.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                for word in paragraph.words:\n",
    "                    for symbol in word.symbols:\n",
    "                        if (feature == FeatureType.SYMBOL):\n",
    "                            bounds.append(symbol.bounding_box)\n",
    "\n",
    "                    if (feature == FeatureType.WORD):\n",
    "                        bounds.append(word.bounding_box)\n",
    "\n",
    "                if (feature == FeatureType.PARA):\n",
    "                    bounds.append(paragraph.bounding_box)\n",
    "\n",
    "            if (feature == FeatureType.BLOCK):\n",
    "                bounds.append(block.bounding_box)\n",
    "\n",
    "        if (feature == FeatureType.PAGE):\n",
    "            bounds.append(block.bounding_box)\n",
    "\n",
    "    # The list `bounds` contains the coordinates of the bounding boxes.\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run some tests to get a feel of the bounding boxes; note that these pop up in a new window \n",
    "render_doc_text(tests_whole['test_2'], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: Experiment with paragraph and block detection \n",
    "- Using out of the box paragraph and block detection on the whole side of the packaging to determine how well that works/which one works better; see examples and discussion. \n",
    "- Made a start at combining bboxes of the words to find better clusters of texts than the out of box functionality. \n",
    "- This is WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_paragraph(path):\n",
    "    \"\"\"\n",
    "    Detects the text in the paragraphs as determined \n",
    "    by the API raw\n",
    "    \"\"\"\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    paragraph_texts = []\n",
    "    for page in response.full_text_annotation.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                #print('paragraph bounding box: ', paragraph.bounding_box)\n",
    "                #print('Paragraph confidence: {}'.format(paragraph.confidence))\n",
    "                paragraph_words = []\n",
    "                for word in paragraph.words:\n",
    "                    word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                    paragraph_words.append(word_text)\n",
    "                paragraph_text = ' '.join(paragraph_words)\n",
    "                paragraph_texts.append(paragraph_text)\n",
    "        return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_block(path):\n",
    "    \"\"\"\n",
    "    Detects the text in the block as determined \n",
    "    by the API raw\n",
    "    \"\"\"\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    block_texts = []\n",
    "    for page in response.full_text_annotation.pages:\n",
    "        for block in page.blocks:\n",
    "            block_words = []\n",
    "            for paragraph in block.paragraphs:\n",
    "                #print('paragraph bounding box: ', paragraph.bounding_box)\n",
    "                #print('Paragraph confidence: {}'.format(paragraph.confidence))\n",
    "                paragraph_words = []\n",
    "                for word in paragraph.words:\n",
    "                    word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                    paragraph_words.append(word_text)\n",
    "                paragraph_text = ' '.join(paragraph_words)\n",
    "                block_words.append(paragraph_text)\n",
    "            block_text = ' '.join(block_words)\n",
    "            block_texts.append(block_text)\n",
    "    return block_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with some examples to see if the block or the paragraph functionality of the API is generally better\n",
    "paragraph_texts = detect_paragraph(tests_whole['test_40'])\n",
    "block_texts = detect_block(tests_whole['test_40'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLMANNS LIGHT REDUCED CALORIE MAYONNAISE TRY OUR RANGE OF SAUCES FOR MORE GREAT FLAVOUR :',\n",
       " \"HELMANN ' S\",\n",
       " 'SMOKEY BBQ',\n",
       " 'SAUCE Ingredients : water , rapeseed oil ( 25 % ) , spirit vine',\n",
       " 'anne EGG yolk ( 1 . 5 % ) , cream powder ( MILK ) , citrus fibre , thickeners ( a',\n",
       " 'CHUNKY KETCHUP',\n",
       " 'BURGER SWEETENED WITH HONEY',\n",
       " 'SAUCE seed oil ( 25 % ) , spirit vinegar , modified corn starch , sugar , salt , free',\n",
       " '( MILK ) , citrus fibre , thickeners ( guar gum , xanthan gum ) , emon juice concentrate , antioxidant ( calcium disodium EDTA ) , natural MUSTARD favouring , paprika extract , sunflower oil . A good source of Omega 3',\n",
       " \"stainably sourced oils . For more info , Unilever UK , Hellmann ' s . Committed to sustainably sourced oils For more\",\n",
       " 'Freepost ADM 3940 London visit www . hellmanns . co . uk or www . hellmanns . ie .',\n",
       " \"SW1A 1YR . 60 % less calories than Hellmann ' s Real Mayonnaise\",\n",
       " 'Unilever Ireland , 20 Riverwalk , . . . . . . NUTRITION INFORMATION . . . . . . . . . . . . . . . . livnical Values ) Per 100g Per portion * * % * per portion * * Citywest , Dublin 24 .',\n",
       " '2 % Your comments count ! Energy 1108kJ / 264kcal 166kJ / 40kcal 269',\n",
       " '6 % FREEPHONE ( UK ) 0800 435562 of which saturates 2 . 6g',\n",
       " '0 . 49',\n",
       " 'OR CALLSAVE ( IE ) 1850 540550 Carbohydrates 6 . 19 0 . 99 < 1 % Mon - Fri 8am - 6pm of which sugars',\n",
       " '< 1 % < 0 . 5g 2 . 39',\n",
       " \"HELLMANN ' S , Unilever and the U < 0 . 50 < 0 . 59 < 1 % 1 . 79 0 . 265 4 %\",\n",
       " 'device are registered trademarks . Omega 3',\n",
       " '2 . 39 0 . 35g * h of Reference intake of an average adult ( 8400kJ / 2000kcal ) * * 1 portion = 15g ( pack contains approx 28 portions )',\n",
       " '3 . 99',\n",
       " '20 %',\n",
       " 'Protein',\n",
       " 'CARIAN',\n",
       " 'VEGETA',\n",
       " 'PEAN VE',\n",
       " 'EUROPA',\n",
       " 'UNION',\n",
       " 'CABEL',\n",
       " 'From plant sources 40 CALORIES PER TABLESPOON .',\n",
       " 'Unilever lo open : Unscrew cap , remove seal , replace cap and flip open lid . REFRIGERATE AFTER OPENING , USE WITHIN 3 MONTHS . DO NUTA',\n",
       " 'VEGETARIAN',\n",
       " '67315014',\n",
       " '2ge ( 430ml )',\n",
       " '81722700479451 \" > FEB 20 HO 3 : 56 L9035C J099']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLMANNS LIGHT REDUCED CALORIE MAYONNAISE TRY OUR RANGE OF SAUCES FOR MORE GREAT FLAVOUR :',\n",
       " \"HELMANN ' S\",\n",
       " 'SMOKEY BBQ',\n",
       " 'SAUCE Ingredients : water , rapeseed oil ( 25 % ) , spirit vine',\n",
       " 'anne EGG yolk ( 1 . 5 % ) , cream powder ( MILK ) , citrus fibre , thickeners ( a',\n",
       " \"CHUNKY KETCHUP BURGER SWEETENED WITH HONEY SAUCE seed oil ( 25 % ) , spirit vinegar , modified corn starch , sugar , salt , free ( MILK ) , citrus fibre , thickeners ( guar gum , xanthan gum ) , emon juice concentrate , antioxidant ( calcium disodium EDTA ) , natural MUSTARD favouring , paprika extract , sunflower oil . A good source of Omega 3 stainably sourced oils . For more info , Unilever UK , Hellmann ' s . Committed to sustainably sourced oils For more Freepost ADM 3940 London visit www . hellmanns . co . uk or www . hellmanns . ie . SW1A 1YR . 60 % less calories than Hellmann ' s Real Mayonnaise Unilever Ireland , 20 Riverwalk , . . . . . . NUTRITION INFORMATION . . . . . . . . . . . . . . . . livnical Values ) Per 100g Per portion * * % * per portion * * Citywest , Dublin 24 . 2 % Your comments count ! Energy 1108kJ / 264kcal 166kJ / 40kcal 269 6 % FREEPHONE ( UK ) 0800 435562 of which saturates 2 . 6g 0 . 49 OR CALLSAVE ( IE ) 1850 540550 Carbohydrates 6 . 19 0 . 99 < 1 % Mon - Fri 8am - 6pm of which sugars < 1 % < 0 . 5g 2 . 39 HELLMANN ' S , Unilever and the U < 0 . 50 < 0 . 59 < 1 % 1 . 79 0 . 265 4 % device are registered trademarks . Omega 3 2 . 39 0 . 35g * h of Reference intake of an average adult ( 8400kJ / 2000kcal ) * * 1 portion = 15g ( pack contains approx 28 portions )\",\n",
       " '3 . 99',\n",
       " '20 %',\n",
       " 'Protein',\n",
       " 'CARIAN',\n",
       " 'VEGETA',\n",
       " 'PEAN VE',\n",
       " 'EUROPA',\n",
       " 'UNION',\n",
       " 'CABEL',\n",
       " 'From plant sources 40 CALORIES PER TABLESPOON . Unilever lo open : Unscrew cap , remove seal , replace cap and flip open lid . REFRIGERATE AFTER OPENING , USE WITHIN 3 MONTHS . DO NUTA',\n",
       " 'VEGETARIAN',\n",
       " '67315014',\n",
       " '2ge ( 430ml )',\n",
       " '81722700479451 \" > FEB 20 HO 3 : 56 L9035C J099']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the results, we notice that neither the paragraph nor the block option give very good results as it is. We start exploring the option of manually combining text based on the bounding boxes of the individual words. Some first attempts at exploring this route below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_bounds(image_file, feature=FeatureType.WORD):\n",
    "    \"\"\"\n",
    "    Returns the bounding boxes of words together with the words \n",
    "    \"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    bounds = []\n",
    "\n",
    "    with io.open(image_file, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = types.Image(content=content)\n",
    "\n",
    "    response = client.document_text_detection(image=image)\n",
    "    document = response.full_text_annotation\n",
    "    \n",
    "    words = []\n",
    "\n",
    "    # Collect specified feature bounds by enumerating all document features\n",
    "    for page in document.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                for word in paragraph.words:\n",
    "                    word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                    words.append(word_text)\n",
    "                    for symbol in word.symbols:\n",
    "                        if (feature == FeatureType.SYMBOL):\n",
    "                            bounds.append(symbol.bounding_box)\n",
    "\n",
    "                    if (feature == FeatureType.WORD):\n",
    "                        bounds.append(word.bounding_box)\n",
    "\n",
    "    # The list `bounds` contains the coordinates of the bounding boxes.\n",
    "    return bounds, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox mergning \n",
    "WIP; currently can just merge any given set of boxes and this works quite well; However, note that the format of the bboxes returned by the API is different to what is expected by the handy CV functions below, so make sure to convert. The box merging works quite well. If we want to use this we will need to define criteria for which boxes should be merged, however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xywh(image_file, example_number, feature=FeatureType.WORD): \n",
    "    bounds, _ = get_word_bounds(image_file, feature)\n",
    "    x = bounds[example_number].vertices[0].x \n",
    "    y = bounds[example_number].vertices[0].y\n",
    "    w = bounds[example_number].vertices[1].x - bounds[example_number].vertices[0].x\n",
    "    h = bounds[example_number].vertices[2].y - bounds[example_number].vertices[1].y\n",
    "    return [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conbined_box(contourRects):\n",
    "    \"\"\"\n",
    "    Takes an array of [x, y, w, h] points and returns \n",
    "    the coordinates of the 4 outer points of the \n",
    "    combined box; note that x and y are the lower \n",
    "    left corner\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    for x,y,w,h in contourRects:\n",
    "        arr.append((x,y))\n",
    "        arr.append((x+w,y+h))\n",
    "\n",
    "    box = cv.minAreaRect(np.asarray(arr))\n",
    "    pts = cv.boxPoints(box) # 4 outer corners\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test - try on just the first 3 words \n",
    "Result: running the below it seems like this works - the words are combined together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = tests_whole['test_1']\n",
    "bounds, words = get_word_bounds(filein, FeatureType.WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "contourRects= np.array([get_xywh(filein, 0), get_xywh(filein, 1), get_xywh(filein, 2)])\n",
    "pts = get_conbined_box(contourRects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(filein)\n",
    "draw = ImageDraw.Draw(image)\n",
    "draw.polygon(pts, None, 'blue')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests \n",
    "Going over the test images to get a feel for what needs to be done to improve the text and bounding detection. It seems like the block detection works slightly better than paragraph, so we use this at the start. \n",
    "\n",
    "### observations on tests - whole\n",
    "- It's not perfect with numbers and this can be a problem with the nutrition info; sometimes it separates just a few numbers from the rest of the block or paragraph and sometimes it doesn't even detect them correctly e.g. the letter 'g' or '.' \n",
    "- Lots of issues with block collection even for the ingredients, e.g. look at test_2, where the ingredients are split even though they are on consecutive lines.\n",
    "- test_4 fails miserably: doesn't detect the nutrition box and messes up the pargraphs; having issues with properly detecting text where there are columns and boxes; unless these are on a colourful background I think. \n",
    "- test_3 and test_7 work quite well; when the box layout is clean and the there is no distortion on the photo. \n",
    "\n",
    "### observations on tests - partial \n",
    "- Using www.world.openfoodfacts.org \n",
    "- Small sample of just ingredients photos to see how well this is doing on clean pictures\n",
    "- Observations: Curved surfaces are very tricky as it splits into multiple weird boxes; there should be some algs for flattening out the images first; also worth playing with contrast or colours to see if this enhances box detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_photos(photo_num, whole=True):\n",
    "    # whole is True for the whole images folder and False for the partial ones\n",
    "    if whole: \n",
    "        img_name = 'test_images/whole/test_' + str(photo_num) + '.jpg'\n",
    "        render_doc_text(img_name, 'test_images/whole/annotated_test_' + str(photo_num) + '.jpg', block=True)\n",
    "    else: \n",
    "        img_name = 'test_images/partial/test_' + str(photo_num) + '.jpg'\n",
    "        render_doc_text(img_name, 'test_images/partial/annotated_test_' + str(photo_num) + '.jpg', block=True)\n",
    "    paragraph_texts = detect_paragraph(img_name)\n",
    "    block_texts = detect_block(img_name)\n",
    "    return block_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['200mle',\n",
       " 'Beiersdorf Beiersdorf AG , D - 20245 Hamburg Art . 89050 www . NIVEA . com Ingredients : Aqua , Glycerin , Paraffinum Liquidum . Myristyl Alcohol , Butylene Glycol , Alcohol Denat . , Stearic Acid , Myristyl Myristate , Cera Microcristallina , Glyceryl Stearate , Hydrogenated Coco - Glycerides , Simmondsia Chinensis Seed Oil , Tocopheryl Acetate , Lanolin Alcohol ( Eucerito ) . Polyglyceryl - 2 Caprate , Dimethicone , Sodium Carbomer , Phenoxyethanol , Linalool , Citronellol , Alpha - Isomethyllonone , Butylphenyl Methylpropional , Limonene , Benzyl Alcohol , Benzyl Salicylate , Parfum Beiersdorf UK Ltd . , Birmingham 637 7YS . RSA : Beiersdort , 21 Lighthouse Road , Umhlanga , 4319 , RSA Consumer Careline : 0860 102091 . Beiersdorf Australia Ltd . , 4 Khartoum Road , North Ryde , NSW , 2113 . NZ : Freephone : 0800 696 483 . Made in Spain',\n",
       " '12M',\n",
       " 'Germany Beiersdorf AG , = reg . tm . of',\n",
       " '89050 . 450 . AD . 05',\n",
       " '5 \" 025970 \" 022574 \" 81224574']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_photos(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients detection \n",
    "- Based on observations above, try to detect the ingredients using keyword search and taking out the block that contains that word.\n",
    "- There will often be other data in the same block and we need to think about how to filter that out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest possible option where we just look for the word 'ingredients'\n",
    "# and retrieve everything after it \n",
    "\n",
    "def find_text_after_word(text, word='ingredients'):\n",
    "    '''\n",
    "    looks for the word 'ingredients' in the \n",
    "    lowercased text and returns all the text \n",
    "    after this word\n",
    "    '''\n",
    "    if word in text.lower():\n",
    "        loc_word = text.lower().find(word)\n",
    "        # adding the +1 as there is often : or space after \n",
    "        # the word 'ingredients'\n",
    "        text_after_word = text[loc_word+len(word)+1:]\n",
    "        # stripping space and : once more just in case\n",
    "        # and '.' at the end as there is often a sent end\n",
    "        return text_after_word.strip().lstrip(':').rstrip('.')\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "including the following:\n",
      "salt and sugar\n",
      " Aqua , Sodium Lauroyl Glyci\n"
     ]
    }
   ],
   "source": [
    "# tests \n",
    "print(find_text_after_word('This contains ingredients including the following:'))\n",
    "print(find_text_after_word('The Ingredients: salt and sugar'))\n",
    "print(find_text_after_word('INGREDIENTS : Aqua , Sodium Lauroyl Glyci'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to extract a list of the ingredients; simplest possible is separating by comma\n",
    "def split_ingredients(text):\n",
    "    '''\n",
    "    returns a list of space-stripped text separated \n",
    "    by commas from the original text\n",
    "    '''\n",
    "    text_components = text.split(',')\n",
    "    return [component.strip() for component in text_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suppose it looks like this: water',\n",
       " 'sugar',\n",
       " 'salt',\n",
       " 'additives ( such as E872 which is not a real thing)']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ingredients('suppose it looks like this: water, sugar , salt , additives ( such as E872 which is not a real thing) ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### very simple ingredients extraction flow: \n",
    "1. Get text in blocks in a list.\n",
    "2. For each element in list list check for 'ingredients'.\n",
    "3. Extract string that contains 'ingredients' if it exists and \n",
    "4. pass it to the splitting function that splits into individual ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients_list(img):\n",
    "    block_texts = detect_block(img)\n",
    "    for block in block_texts:\n",
    "        ingredients = find_text_after_word(block)\n",
    "        if ingredients != '':\n",
    "            individual_ingredients = split_ingredients(ingredients)\n",
    "            return individual_ingredients\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the partial images i.e. the ingredients list from OFF \n",
    "ingredients_partial = {}\n",
    "for img in os.listdir(os.path.join('test_images', 'partial')):\n",
    "    if img.startswith('test_'):\n",
    "        img_path = os.path.join('test_images', 'partial', img)\n",
    "        ingredients_partial[img] = extract_ingredients_list(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INGREDIENTS : Rice ( 44 % )',\n",
       " 'Wholewheat ( 35 % )',\n",
       " 'Sugar',\n",
       " 'Barley ( 4 . 5 % )',\n",
       " 'Freeze dried fruits ( 4 . 5 % ) ( Strawberry',\n",
       " 'Cherry )',\n",
       " 'Malted barley flour ( 3 . 5 % )',\n",
       " 'Barley malt flavouring',\n",
       " '']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the results \n",
    "ingredients_partial['test_20.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on the full images i.e. own package photos \n",
    "ingredients_whole = {}\n",
    "for img in os.listdir(os.path.join('test_images', 'whole')):\n",
    "    if img.startswith('test_'):\n",
    "        img_path = os.path.join('test_images', 'whole', img)\n",
    "        ingredients_whole[img] = extract_ingredients_list(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. Stop use and ask a dentist if oral irritation occurs . Keep out of the reach of children . Made in Italy . LISTERINE® is a registered trade mark . Lot number : see bottom of the bottle JOHNSON & JOHNSON GmbH D - 41470 Neuss . DE JOHNSON & JOHNSON LIMITED Maidenhead',\n",
       " 'UK',\n",
       " 'SL6 3UG Careline : 0808 238 9999 JOHNSON & JOHNSON ( IRELAND ) LIMITED Airton Road',\n",
       " 'Tallaght',\n",
       " 'Dublin 24',\n",
       " 'Ireland . Careline : 1800 22 0044 PR - 017429 ] - INGREDIENTS : Aqua',\n",
       " 'Alcohol',\n",
       " 'Sorbitol',\n",
       " 'Poloxamer 407',\n",
       " 'Benzoic Acid',\n",
       " 'nic Chloride',\n",
       " 'Eucalyptol Aroma',\n",
       " 'Sodium Saccharin',\n",
       " 'Methyl Salicylate',\n",
       " 'Thymol',\n",
       " 'Menthol',\n",
       " 'Sodium Fluoride',\n",
       " 'Sodium Benzoate',\n",
       " 'Sucralose',\n",
       " 'Propylene Glycol',\n",
       " 'CI 16035',\n",
       " 'CI 42090']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_whole['test_48.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
